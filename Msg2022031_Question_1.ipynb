{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Question 1\n",
        "###1(a)"
      ],
      "metadata": {
        "id": "YIIh3xTwYgSg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khaWiCEAX_Wq",
        "outputId": "86b212fc-4a41-4723-9603-57811735085e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Doing batch gradient without feature scaling\n",
            "[-0.003757210025843233, 0.011335268594940533, -0.00544624335125886]\n",
            "Accuracy is : 82.75862068965517\n",
            "Doing stochaistic gradient without feature scaling\n",
            "[-17.00941944986729, 0.1507787664831176, 0.16322765871133627]\n",
            "Accuracy is : 89.65517241379311\n",
            "Doing Mini batch gradient without feature scaling\n",
            "[-0.0032175980479680713, 0.008659553033513023, -0.0035662510332845954]\n",
            "Accuracy is : 82.75862068965517\n",
            "Doing batch gradient with feature scaling\n",
            "[0.0007110721689603617, 0.004233063763920973, 0.003164762268784069]\n",
            "Accuracy is : 93.33333333333333\n",
            "Doing stochaistic gradient with feature scaling\n",
            "[0.579981287203592, 7.806737165995212, 6.594950012053912]\n",
            "Accuracy is : 83.33333333333334\n",
            "Doing Mini batch gradient with feature scaling\n",
            "[1.0929605679745887e-06, 0.003365429126505297, 0.002850656019519231]\n",
            "Accuracy is : 86.66666666666667\n"
          ]
        }
      ],
      "source": [
        "# In this code I have implemented logistic regression using batch gradient, stochaistic gradient and mini batch gradient for feature scaled and unscaled data.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import math\n",
        "\n",
        "def getdata():\n",
        "\tinput_data = pd.read_csv(\"/content/marks.csv\")\n",
        "\tY = input_data['selected']\n",
        "\tmarks1 = input_data['marks1']\n",
        "\tmarks2 = input_data['marks2']\n",
        "\n",
        "\tX_train = []\n",
        "\tX_test = []\n",
        "\tY_train = []\n",
        "\tY_test = []\n",
        "\tfor i in range(70):\n",
        "\t\tX_train.append([1, marks1[i], marks2[i]])\n",
        "\t\tY_train.append(Y[i])\n",
        "\n",
        "\tfor i in range(71, 100):\n",
        "\t\tX_test.append([1, marks1[i], marks2[i]])\n",
        "\t\tY_test.append(Y[i])\n",
        "\treturn X_train, X_test, Y_train, Y_test\n",
        "\n",
        "def getscaleddata():\n",
        "\tinput_data = pd.read_csv(\"marks.csv\")\n",
        "\tY = input_data['selected']\n",
        "\tmarks1 = input_data['marks1']\n",
        "\tmarks2 = input_data['marks2']\n",
        "\n",
        "\tmeanmarks1 = np.mean(marks1)\n",
        "\tmaxmarks1 = np.max(marks1)\n",
        "\tminmarks1 = np.min(marks1)\n",
        "\n",
        "\tmeanmarks2 = np.mean(marks2)\n",
        "\tmaxmarks2 = np.max(marks2)\n",
        "\tminmarks2 = np.min(marks2)\n",
        "\n",
        "\tX_train = []\n",
        "\tX_test = []\n",
        "\tY_train = []\n",
        "\tY_test = []\n",
        "\n",
        "\tfor i in range(70):\n",
        "\t\tX_train.append([1, (marks1[i] - meanmarks1) / (maxmarks1 - minmarks1), (marks2[i] - meanmarks2) / (maxmarks2 - minmarks2)])\n",
        "\t\tY_train.append(Y[i])\n",
        "\n",
        "\tfor i in range(70, 100):\n",
        "\t\tX_test.append([1, (marks1[i] - meanmarks1) / (maxmarks1 - minmarks1), (marks2[i] - meanmarks2) / (maxmarks2 - minmarks2)])\n",
        "\t\tY_test.append(Y[i])\n",
        "\treturn X_train, X_test, Y_train, Y_test\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1 + math.exp(-1 * z))\n",
        "\n",
        "# Function to calculate Slope to find coefficients\n",
        "def Slope(Coeff, X_train, Y_train, ind):\n",
        "\tdiff = 0\n",
        "\tfor i in range(len(X_train)):\n",
        "\t\titr = 0\n",
        "\t\tfor j in range(len(Coeff)):\n",
        "\t\t\titr = itr + Coeff[j] * X_train[i][j]\n",
        "\t\tdiff += (sigmoid(itr) - Y_train[i]) * X_train[i][ind]\n",
        "\treturn diff\n",
        "\n",
        "# Using batch gradient\n",
        "def batchgra(X_train, Y_train, alpha = 0.00001, epochs = 50000):\n",
        "\tLearningRateNoScaling = alpha\n",
        "\n",
        "\tCoeff = [0, 0, 0]\n",
        "\tlis1 = []\n",
        "\tfor i in range(epochs):\n",
        "\t\tTempCoeff = Coeff.copy()\n",
        "\t\tfor j in range(len(Coeff)):\n",
        "\t\t\tTempCoeff[j] = TempCoeff[j] - ((LearningRateNoScaling / len(X_train)) * (Slope(Coeff, X_train, Y_train, j)))\n",
        "\t\tCoeff = TempCoeff.copy()\n",
        "\treturn Coeff\n",
        "\n",
        "# Finding Accuracy\n",
        "def printaccuracy(X_test, Y_test, Coeff):\n",
        "\tcount = 0\n",
        "\tfor i in range(len(X_test)):\n",
        "\t\tpredicted = 0\n",
        "\t\tfor j in range(len(Coeff)):\n",
        "\t\t  \tpredicted = predicted + Coeff[j] * X_test[i][j]\n",
        "\t\tpredicted = sigmoid(predicted)\n",
        "\t\tif predicted > 0.5:\n",
        "\t\t\tif Y_test[i] == 1:\n",
        "\t\t\t\tcount += 1\n",
        "\t\telse:\n",
        "\t\t\tif Y_test[i] == 0:\n",
        "\t\t\t\tcount += 1\n",
        "\tprint(\"Accuracy is : \" + str(count / len(Y_test) * 100))\n",
        "\n",
        "def SlopeStoch(Coeff, X_train, ActualVal, ind):\n",
        "\titr = 0\n",
        "\tfor j in range(len(Coeff)):\n",
        "\t\titr = itr + Coeff[j] * X_train[j]\n",
        "\treturn (sigmoid(itr) - ActualVal) * X_train[ind]\n",
        "\n",
        "def stochgra(X_train, Y_train, alpha = 0.00001, epochs = 50000):\n",
        "\tLearningRateNoScaling = alpha\n",
        "\tCoeff = [0, 0, 0]\n",
        "\tfor iter in range(epochs):\n",
        "\t\tfor i in range(len(Y_train)):\n",
        "\t\t\tTempCoeff = Coeff.copy()\n",
        "\t\t\tfor j in range(3):\n",
        "\t\t\t\tTempCoeff[j] = TempCoeff[j] - (LearningRateNoScaling * (SlopeStoch(Coeff, X_train[i], Y_train[i], j)))\n",
        "\t\t\tCoeff = TempCoeff.copy()\n",
        "\treturn Coeff\n",
        "\n",
        "def minibtchgra(X_train, Y_train, alpha = 0.000000001, epochs = 30, batchsize = 20):\n",
        "\tLearningRateScaling = alpha\n",
        "\tCoeff = [0, 0, 0]\n",
        "\tNoOfBatches = math.ceil(len(Y_train) / batchsize)\n",
        "\tequallyDiv = False\n",
        "\tif (len(Y_train) % batchsize == 0):\n",
        "\t\tequallyDiv = True;\n",
        "\n",
        "\tfor epoch in range(epochs):\n",
        "\t\tfor batch in range(NoOfBatches):\n",
        "\t\t\tSummation = [0, 0, 0]\n",
        "\t\t\tfor j in range(len(Coeff)):\n",
        "\t\t\t\tfor i in range(batchsize):\n",
        "\t\t\t\t\tif (batch * batchsize + i == len(X_train)):\n",
        "\t\t\t\t\t\tbreak\n",
        "\t\t\t\t\tPredictedValue = 0.0\n",
        "\t\t\t\t\tfor wj in range(len(Coeff)):\n",
        "\t\t\t\t\t\tPredictedValue += Coeff[wj] * X_train[batch * batchsize + i][wj]\n",
        "\t\t\t\t\tPredictedValue = sigmoid(PredictedValue)\n",
        "\t\t\t\t\tPredictedValue -= Y_train[batch * batchsize + i]\n",
        "\t\t\t\t\tPredictedValue *= X_train[batch * batchsize + i][j]\n",
        "\t\t\t\t\tSummation[j] += PredictedValue;\n",
        "\n",
        "\t\t\tif (not equallyDiv and batch == NoOfBatches - 1):\n",
        "\t\t\t\tfor j in range(len(Summation)):\n",
        "\t\t\t\t\tCoeff[j] -= (Summation[j] / (len(Y_train) % batchsize)) * LearningRateScaling\n",
        "\t\t\telse:\n",
        "\t\t\t\tfor j in range(len(Summation)):\n",
        "\t\t\t\t\tCoeff[j] -= (Summation[j] / batchsize) * LearningRateScaling\n",
        "\treturn Coeff\n",
        "\n",
        "# First doing batch gradient, stochaistic gradient and mini batch gradient without feature scaling.\n",
        "X_train, X_test, Y_train, Y_test = getdata()\n",
        "\n",
        "print(\"Doing batch gradient without feature scaling\")\n",
        "coeff = batchgra(X_train, Y_train, 0.00001, 5000)\n",
        "print(coeff)\n",
        "printaccuracy(X_test, Y_test, coeff)\n",
        "\n",
        "print(\"Doing stochaistic gradient without feature scaling\")\n",
        "coeff = stochgra(X_train, Y_train, 0.001, 5000)\n",
        "print(coeff)\n",
        "printaccuracy(X_test, Y_test, coeff)\n",
        "\n",
        "print(\"Doing Mini batch gradient without feature scaling\")\n",
        "coeff = minibtchgra(X_train, Y_train, 0.0001, 100, 20)\n",
        "print(coeff)\n",
        "printaccuracy(X_test, Y_test, coeff)\n",
        "\n",
        "# Now doing batch gradient, stochaistic gradient and mini batch gradient with feature scaling.\n",
        "X_train, X_test, Y_train, Y_test = getscaleddata()\n",
        "\n",
        "print(\"Doing batch gradient with feature scaling\")\n",
        "coeff = batchgra(X_train, Y_train, 0.00001, 5000)\n",
        "print(coeff)\n",
        "printaccuracy(X_test, Y_test, coeff)\n",
        "\n",
        "print(\"Doing stochaistic gradient with feature scaling\")\n",
        "coeff = stochgra(X_train, Y_train, 0.001, 5000)\n",
        "print(coeff)\n",
        "printaccuracy(X_test, Y_test, coeff)\n",
        "\n",
        "print(\"Doing Mini batch gradient with feature scaling\")\n",
        "coeff = minibtchgra(X_train, Y_train, 0.0001, 100, 20)\n",
        "print(coeff)\n",
        "printaccuracy(X_test, Y_test, coeff)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1(b)"
      ],
      "metadata": {
        "id": "NyX_4XG6YyJb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In this code I have implemented logistic regression using batch gradient, stochaistic gradient and mini batch gradient for feature scaled and unscaled data.\n",
        "# Here I have also used higher powers of the data to make more features.\n",
        "# Now the hypothesis looks like h(x) = g(wx) where g(wx) = 1 / (1 + e^(-wx)) and wx = w0 + w1x + w2y + w3x^2 + w4y^2 + w5xy + w6x^3 + w7y^3 + w8x^2y + w9xy^2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import math\n",
        "\n",
        "def getdata():\n",
        "\tinput_data = pd.read_csv(\"/content/marks.csv\")\n",
        "\tY = input_data['selected']\n",
        "\tmarks1 = input_data['marks1']\n",
        "\tmarks2 = input_data['marks2']\n",
        "\tmarks1sq = []\n",
        "\tfor i in marks1:\n",
        "\t\tmarks1sq.append(i * i)\n",
        "\n",
        "\tX_train = []\n",
        "\tX_test = []\n",
        "\tY_train = []\n",
        "\tY_test = []\n",
        "\tfor i in range(70):\n",
        "\t\tX_train.append([1, marks1[i], marks2[i], marks1[i] * marks1[i], marks2[i] * marks2[i], marks1[i] * marks2[i], marks1[i] * marks1[i] * marks1[i], marks2[i] * marks2[i] * marks2[i], marks1[i] * marks1[i] * marks2[i], marks1[i] * marks2[i] * marks2[i]])\n",
        "\t\tY_train.append(Y[i])\n",
        "\n",
        "\tfor i in range(70, 100):\n",
        "\t\tX_test.append([1, marks1[i], marks2[i], marks1[i] * marks1[i], marks2[i] * marks2[i], marks1[i] * marks2[i], marks1[i] * marks1[i] * marks1[i], marks2[i] * marks2[i] * marks2[i], marks1[i] * marks1[i] * marks2[i], marks1[i] * marks2[i] * marks2[i]])\n",
        "\t\tY_test.append(Y[i])\n",
        "\treturn X_train, X_test, Y_train, Y_test\n",
        "\n",
        "def getscaleddata():\n",
        "\tinput_data = pd.read_csv(\"marks.csv\")\n",
        "\tY = input_data['selected']\n",
        "\tmarks1 = input_data['marks1']\n",
        "\tmarks2 = input_data['marks2']\n",
        "\n",
        "\tmeanmarks1 = np.mean(marks1)\n",
        "\tmaxmarks1 = np.max(marks1)\n",
        "\tminmarks1 = np.min(marks1)\n",
        "\n",
        "\tmeanmarks2 = np.mean(marks2)\n",
        "\tmaxmarks2 = np.max(marks2)\n",
        "\tminmarks2 = np.min(marks2)\n",
        "\n",
        "\tmarks1sq = []\n",
        "\tfor i in marks1:\n",
        "\t\tmarks1sq.append(i * i)\n",
        "\tmeanmarks1sq = np.mean(marks1sq)\n",
        "\tmaxmarks1sq = np.max(marks1sq)\n",
        "\tminmarks1sq = np.min(marks1sq)\n",
        "\tfor i in range(len(marks1sq)):\n",
        "\t\tmarks1sq[i] = (marks1sq[i] - meanmarks1sq) / (maxmarks1sq - minmarks1sq)\n",
        "\n",
        "\tmarks2sq = []\n",
        "\tfor i in marks2:\n",
        "\t\tmarks2sq.append(i * i)\n",
        "\tmeanmarks2sq = np.mean(marks2sq)\n",
        "\tmaxmarks2sq = np.max(marks2sq)\n",
        "\tminmarks2sq = np.min(marks2sq)\n",
        "\tfor i in range(len(marks2sq)):\n",
        "\t\tmarks2sq[i] = (marks2sq[i] - meanmarks2sq) / (maxmarks2sq - minmarks2sq)\n",
        "\n",
        "\tmarks1marks2 = []\n",
        "\tfor i in range(len(marks1)):\n",
        "\t\tmarks1marks2.append(marks1[i] * marks2[i])\n",
        "\tmeanmarks1marks2 = np.mean(marks1marks2)\n",
        "\tmaxmarks1marks2 = np.max(marks1marks2)\n",
        "\tminmarks1marks2 = np.min(marks1marks2)\n",
        "\tfor i in range(len(marks1marks2)):\n",
        "\t\tmarks1marks2[i] = (marks1marks2[i] - meanmarks1marks2) / (maxmarks1marks2 - minmarks1marks2)\n",
        "\n",
        "\tmarks1cu = []\n",
        "\tfor i in marks1:\n",
        "\t\tmarks1cu.append(i * i * i)\n",
        "\tmeanmarks1cu = np.mean(marks1cu)\n",
        "\tmaxmarks1cu = np.max(marks1cu)\n",
        "\tminmarks1cu = np.min(marks1cu)\n",
        "\tfor i in range(len(marks1cu)):\n",
        "\t\tmarks1cu[i] = (marks1cu[i] - meanmarks1cu) / (maxmarks1cu - minmarks1cu)\n",
        "\n",
        "\tmarks2cu = []\n",
        "\tfor i in marks2:\n",
        "\t\tmarks2cu.append(i * i * i)\n",
        "\tmeanmarks2cu = np.mean(marks2cu)\n",
        "\tmaxmarks2cu = np.max(marks2cu)\n",
        "\tminmarks2cu = np.min(marks2cu)\n",
        "\tfor i in range(len(marks2cu)):\n",
        "\t\tmarks2cu[i] = (marks2cu[i] - meanmarks2cu) / (maxmarks2cu - minmarks2cu)\n",
        "\n",
        "\tmarks1sqmarks2 = []\n",
        "\tfor i in range(len(marks1)):\n",
        "\t\tmarks1sqmarks2.append(marks1[i] * marks1[i] * marks2[i])\n",
        "\tmeanmarks1sqmarks2 = np.mean(marks1sqmarks2)\n",
        "\tmaxmarks1sqmarks2 = np.max(marks1sqmarks2)\n",
        "\tminmarks1sqmarks2 = np.min(marks1sqmarks2)\n",
        "\tfor i in range(len(marks1sqmarks2)):\n",
        "\t\tmarks1sqmarks2[i] = (marks1sqmarks2[i] - meanmarks1sqmarks2) / (maxmarks1sqmarks2 - minmarks1sqmarks2)\n",
        "\n",
        "\tmarks2sqmarks1 = []\n",
        "\tfor i in range(len(marks1)):\n",
        "\t\tmarks2sqmarks1.append(marks1[i] * marks2[i] * marks2[i])\n",
        "\tmeanmarks2sqmarks1 = np.mean(marks2sqmarks1)\n",
        "\tmaxmarks2sqmarks1 = np.max(marks2sqmarks1)\n",
        "\tminmarks2sqmarks1 = np.min(marks2sqmarks1)\n",
        "\tfor i in range(len(marks2sqmarks1)):\n",
        "\t\tmarks2sqmarks1[i] = (marks2sqmarks1[i] - meanmarks2sqmarks1) / (maxmarks2sqmarks1 - minmarks2sqmarks1)\n",
        "\n",
        "\tX_train = []\n",
        "\tX_test = []\n",
        "\tY_train = []\n",
        "\tY_test = []\n",
        "\n",
        "\tfor i in range(70):\n",
        "\t\tX_train.append([1, (marks1[i] - meanmarks1) / (maxmarks1 - minmarks1), (marks2[i] - meanmarks2) / (maxmarks2 - minmarks2), marks1sq[i], marks2sq[i], marks1marks2[i], marks1cu[i], marks2cu[i], marks1sqmarks2[i], marks2sqmarks1[i]])\n",
        "\t\tY_train.append(Y[i])\n",
        "\n",
        "\tfor i in range(70, 100):\n",
        "\t\tX_test.append([1, (marks1[i] - meanmarks1) / (maxmarks1 - minmarks1), (marks2[i] - meanmarks2) / (maxmarks2 - minmarks2), marks1sq[i], marks2sq[i], marks1marks2[i], marks1cu[i], marks2cu[i], marks1sqmarks2[i], marks2sqmarks1[i]])\n",
        "\t\tY_test.append(Y[i])\n",
        "\treturn X_train, X_test, Y_train, Y_test\n",
        "\n",
        "def sigmoid(z):\n",
        "\ttry:\n",
        "\t\tans = 1.0 / (1 + math.exp(-1 * z))\n",
        "\texcept OverflowError:\n",
        "\t\tans = 0\n",
        "\treturn ans\n",
        "\n",
        "# Function to calculate Slope to find coefficients\n",
        "def Slope(Coeff, X_train, Y_train, ind):\n",
        "\tdiff = 0\n",
        "\tfor i in range(len(X_train)):\n",
        "\t\titr = 0\n",
        "\t\tfor j in range(len(Coeff)):\n",
        "\t\t\titr = itr + Coeff[j] * X_train[i][j]\n",
        "\t\tdiff += (sigmoid(itr) - Y_train[i]) * X_train[i][ind]\n",
        "\treturn diff\n",
        "\n",
        "# Using batch gradient\n",
        "def batchgra(X_train, Y_train, alpha = 0.00001, epochs = 50000):\n",
        "\tLearningRateNoScaling = alpha\n",
        "\n",
        "\tCoeff = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "\tlis1 = []\n",
        "\tfor i in range(epochs):\n",
        "\t\tTempCoeff = Coeff.copy()\n",
        "\t\tfor j in range(len(Coeff)):\n",
        "\t\t\tTempCoeff[j] = TempCoeff[j] - ((LearningRateNoScaling / len(X_train)) * (Slope(Coeff, X_train, Y_train, j)))\n",
        "\t\tCoeff = TempCoeff.copy()\n",
        "\treturn Coeff\n",
        "\n",
        "# Finding Accuracy\n",
        "def printaccuracy(X_test, Y_test, Coeff):\n",
        "\tcount = 0\n",
        "\tfor i in range(len(X_test)):\n",
        "\t\tpredicted = 0\n",
        "\t\tfor j in range(len(Coeff)):\n",
        "\t\t  \tpredicted = predicted + Coeff[j] * X_test[i][j]\n",
        "\t\tpredicted = sigmoid(predicted)\n",
        "\t\tif predicted > 0.5:\n",
        "\t\t\tif Y_test[i] == 1:\n",
        "\t\t\t\tcount += 1\n",
        "\t\telse:\n",
        "\t\t\tif Y_test[i] == 0:\n",
        "\t\t\t\tcount += 1\n",
        "\tprint(\"Accuracy is : \" + str(count / len(Y_test) * 100))\n",
        "\n",
        "def SlopeStoch(Coeff, X_train, ActualVal, ind):\n",
        "\titr = 0\n",
        "\tfor j in range(len(Coeff)):\n",
        "\t\titr = itr + Coeff[j] * X_train[j]\n",
        "\treturn (sigmoid(itr) - ActualVal) * X_train[ind]\n",
        "\n",
        "def stochgra(X_train, Y_train, alpha = 0.00001, epochs = 50000):\n",
        "\tLearningRateNoScaling = alpha\n",
        "\tCoeff = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "\tfor iter in range(epochs):\n",
        "\t\tfor i in range(len(Y_train)):\n",
        "\t\t\tTempCoeff = Coeff.copy()\n",
        "\t\t\tfor j in range(len(Coeff)):\n",
        "\t\t\t\tTempCoeff[j] = TempCoeff[j] - (LearningRateNoScaling * (SlopeStoch(Coeff, X_train[i], Y_train[i], j)))\n",
        "\t\t\tCoeff = TempCoeff.copy()\n",
        "\treturn Coeff\n",
        "\n",
        "def minibtchgra(X_train, Y_train, alpha = 0.000000001, epochs = 30, batchsize = 20):\n",
        "\tLearningRateScaling = alpha\n",
        "\tCoeff = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "\tNoOfBatches = math.ceil(len(Y_train) / batchsize)\n",
        "\tequallyDiv = False\n",
        "\tif (len(Y_train) % batchsize == 0):\n",
        "\t\tequallyDiv = True;\n",
        "\n",
        "\tfor epoch in range(epochs):\n",
        "\t\tfor batch in range(NoOfBatches):\n",
        "\t\t\tSummation = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "\t\t\tfor j in range(len(Coeff)):\n",
        "\t\t\t\tfor i in range(batchsize):\n",
        "\t\t\t\t\tif (batch * batchsize + i == len(X_train)):\n",
        "\t\t\t\t\t\tbreak\n",
        "\t\t\t\t\tPredictedValue = 0.0\n",
        "\t\t\t\t\tfor wj in range(len(Coeff)):\n",
        "\t\t\t\t\t\tPredictedValue += Coeff[wj] * X_train[batch * batchsize + i][wj]\n",
        "\t\t\t\t\tPredictedValue = sigmoid(PredictedValue)\n",
        "\t\t\t\t\tPredictedValue -= Y_train[batch * batchsize + i]\n",
        "\t\t\t\t\tPredictedValue *= X_train[batch * batchsize + i][j]\n",
        "\n",
        "\t\t\t\t\tSummation[j] += PredictedValue;\n",
        "\n",
        "\t\t\tif (not equallyDiv and batch == NoOfBatches - 1):\n",
        "\t\t\t\tfor j in range(len(Summation)):\n",
        "\t\t\t\t\tCoeff[j] -= (Summation[j] / (len(Y_train) % batchsize)) * LearningRateScaling\n",
        "\t\t\telse:\n",
        "\t\t\t\tfor j in range(len(Summation)):\n",
        "\t\t\t\t\tCoeff[j] -= (Summation[j] / batchsize) * LearningRateScaling\n",
        "\treturn Coeff\n",
        "\n",
        "# First doing batch gradient, stochaistic gradient and mini batch gradient without feature scaling.\n",
        "X_train, X_test, Y_train, Y_test = getdata()\n",
        "\n",
        "print(\"Doing batch gradient without feature scaling\")\n",
        "coeff = batchgra(X_train, Y_train, 0.1, 5)\n",
        "print(coeff)\n",
        "printaccuracy(X_test, Y_test, coeff)\n",
        "\n",
        "print(\"Doing stochaistic gradient without feature scaling\")\n",
        "coeff = stochgra(X_train, Y_train, 0.001, 5000)\n",
        "print(coeff)\n",
        "printaccuracy(X_test, Y_test, coeff)\n",
        "\n",
        "print(\"Doing Mini batch gradient without feature scaling\")\n",
        "coeff = minibtchgra(X_train, Y_train, 0.0001, 100, 20)\n",
        "print(coeff)\n",
        "printaccuracy(X_test, Y_test, coeff)\n",
        "\n",
        "# Now doing batch gradient, stochaistic gradient and mini batch gradient with feature scaling.\n",
        "X_train, X_test, Y_train, Y_test = getscaleddata()\n",
        "\n",
        "print(\"Doing batch gradient with feature scaling\")\n",
        "coeff = batchgra(X_train, Y_train, 0.00001, 5000)\n",
        "print(coeff)\n",
        "printaccuracy(X_test, Y_test, coeff)\n",
        "\n",
        "print(\"Doing stochaistic gradient with feature scaling\")\n",
        "coeff = stochgra(X_train, Y_train, 0.001, 5000)\n",
        "print(coeff)\n",
        "printaccuracy(X_test, Y_test, coeff)\n",
        "\n",
        "print(\"Doing Mini batch gradient with feature scaling\")\n",
        "coeff = minibtchgra(X_train, Y_train, 0.0001, 100, 20)\n",
        "print(coeff)\n",
        "printaccuracy(X_test, Y_test, coeff)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5AhfZtWYm8X",
        "outputId": "cd83d43b-40eb-484f-d1b4-c02c60816961"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Doing batch gradient without feature scaling\n",
            "[-0.09285714285714286, -2.962944402749999, -3.793985451964282, -53.50515248428712, -134.6308304192547, -39.087311554567066, 3813.772544774776, -2518.019699730583, 7435.942238581199, 5810.239816051123]\n",
            "Accuracy is : 80.0\n",
            "Doing stochaistic gradient without feature scaling\n",
            "[-43.43849999998947, -1635.0479041192586, -1835.050026480356, -48394.95114315107, -60847.29545445934, -50599.02472198009, -2169.4011320874347, 1744.0833275123011, 7167.509542840378, -3370.3706969044347]\n",
            "Accuracy is : 86.66666666666667\n",
            "Doing Mini batch gradient without feature scaling\n",
            "[-0.009360000000000005, -0.3662235170338253, -0.38015617379400046, -11.706932864893403, -12.345125492678925, -10.675917365065231, -39.32013516042298, -29.371301130926454, 51.51093223523594, 26.343210377432314]\n",
            "Accuracy is : 70.0\n",
            "Doing batch gradient with feature scaling\n",
            "[0.0007163711061252651, 0.004225909451385743, 0.0031584525799369587, 0.003957192704594542, 0.003145436954465893, 0.004711347610800118, 0.0035719954798885687, 0.0029981642737297374, 0.004245121458872718, 0.004684789864973334]\n",
            "Accuracy is : 86.66666666666667\n",
            "Doing stochaistic gradient with feature scaling\n",
            "[1.5042734546507814, 3.028137779226378, 2.127475481029001, 1.7862959476079467, 1.265642813942837, 4.198883942810742, 0.6018855985199819, 0.4665981638564915, 3.2650812220664833, 3.724583877975815]\n",
            "Accuracy is : 83.33333333333334\n",
            "Doing Mini batch gradient with feature scaling\n",
            "[5.6067824154417875e-06, 0.003360601276176679, 0.0028458956292451284, 0.0031411210096455857, 0.0028664655157666567, 0.0038903430569185455, 0.0028288628005613044, 0.0027634201051740352, 0.003435244811929776, 0.003946280853334815]\n",
            "Accuracy is : 73.33333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1(c)"
      ],
      "metadata": {
        "id": "eI1qVbEkZP0t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In this code I have implemented feature scaled logistic regression using batch gradient, stochaistic gradient and mini batch gradient with and without regularisation.\n",
        "# Here I have also used higher powers of the data to make more features.\n",
        "# Now the hypothesis looks like h(x) = g(wx) where g(wx) = 1 / (1 + e^(-wx)) and wx = w0 + w1x + w2y + w3x^2 + w4y^2 + w5xy + w6x^3 + w7y^3 + w8x^2y + w9xy^2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import math\n",
        "\n",
        "def getscaleddata():\n",
        "\tinput_data = pd.read_csv(\"/content/marks.csv\")\n",
        "\tY = input_data['selected']\n",
        "\tmarks1 = input_data['marks1']\n",
        "\tmarks2 = input_data['marks2']\n",
        "\n",
        "\tmeanmarks1 = np.mean(marks1)\n",
        "\tmaxmarks1 = np.max(marks1)\n",
        "\tminmarks1 = np.min(marks1)\n",
        "\n",
        "\tmeanmarks2 = np.mean(marks2)\n",
        "\tmaxmarks2 = np.max(marks2)\n",
        "\tminmarks2 = np.min(marks2)\n",
        "\n",
        "\tmarks1sq = []\n",
        "\tfor i in marks1:\n",
        "\t\tmarks1sq.append(i * i)\n",
        "\tmeanmarks1sq = np.mean(marks1sq)\n",
        "\tmaxmarks1sq = np.max(marks1sq)\n",
        "\tminmarks1sq = np.min(marks1sq)\n",
        "\tfor i in range(len(marks1sq)):\n",
        "\t\tmarks1sq[i] = (marks1sq[i] - meanmarks1sq) / (maxmarks1sq - minmarks1sq)\n",
        "\n",
        "\tmarks2sq = []\n",
        "\tfor i in marks2:\n",
        "\t\tmarks2sq.append(i * i)\n",
        "\tmeanmarks2sq = np.mean(marks2sq)\n",
        "\tmaxmarks2sq = np.max(marks2sq)\n",
        "\tminmarks2sq = np.min(marks2sq)\n",
        "\tfor i in range(len(marks2sq)):\n",
        "\t\tmarks2sq[i] = (marks2sq[i] - meanmarks2sq) / (maxmarks2sq - minmarks2sq)\n",
        "\n",
        "\tmarks1marks2 = []\n",
        "\tfor i in range(len(marks1)):\n",
        "\t\tmarks1marks2.append(marks1[i] * marks2[i])\n",
        "\tmeanmarks1marks2 = np.mean(marks1marks2)\n",
        "\tmaxmarks1marks2 = np.max(marks1marks2)\n",
        "\tminmarks1marks2 = np.min(marks1marks2)\n",
        "\tfor i in range(len(marks1marks2)):\n",
        "\t\tmarks1marks2[i] = (marks1marks2[i] - meanmarks1marks2) / (maxmarks1marks2 - minmarks1marks2)\n",
        "\n",
        "\tmarks1cu = []\n",
        "\tfor i in marks1:\n",
        "\t\tmarks1cu.append(i * i * i)\n",
        "\tmeanmarks1cu = np.mean(marks1cu)\n",
        "\tmaxmarks1cu = np.max(marks1cu)\n",
        "\tminmarks1cu = np.min(marks1cu)\n",
        "\tfor i in range(len(marks1cu)):\n",
        "\t\tmarks1cu[i] = (marks1cu[i] - meanmarks1cu) / (maxmarks1cu - minmarks1cu)\n",
        "\n",
        "\tmarks2cu = []\n",
        "\tfor i in marks2:\n",
        "\t\tmarks2cu.append(i * i * i)\n",
        "\tmeanmarks2cu = np.mean(marks2cu)\n",
        "\tmaxmarks2cu = np.max(marks2cu)\n",
        "\tminmarks2cu = np.min(marks2cu)\n",
        "\tfor i in range(len(marks2cu)):\n",
        "\t\tmarks2cu[i] = (marks2cu[i] - meanmarks2cu) / (maxmarks2cu - minmarks2cu)\n",
        "\n",
        "\tmarks1sqmarks2 = []\n",
        "\tfor i in range(len(marks1)):\n",
        "\t\tmarks1sqmarks2.append(marks1[i] * marks1[i] * marks2[i])\n",
        "\tmeanmarks1sqmarks2 = np.mean(marks1sqmarks2)\n",
        "\tmaxmarks1sqmarks2 = np.max(marks1sqmarks2)\n",
        "\tminmarks1sqmarks2 = np.min(marks1sqmarks2)\n",
        "\tfor i in range(len(marks1sqmarks2)):\n",
        "\t\tmarks1sqmarks2[i] = (marks1sqmarks2[i] - meanmarks1sqmarks2) / (maxmarks1sqmarks2 - minmarks1sqmarks2)\n",
        "\n",
        "\tmarks2sqmarks1 = []\n",
        "\tfor i in range(len(marks1)):\n",
        "\t\tmarks2sqmarks1.append(marks1[i] * marks2[i] * marks2[i])\n",
        "\tmeanmarks2sqmarks1 = np.mean(marks2sqmarks1)\n",
        "\tmaxmarks2sqmarks1 = np.max(marks2sqmarks1)\n",
        "\tminmarks2sqmarks1 = np.min(marks2sqmarks1)\n",
        "\tfor i in range(len(marks2sqmarks1)):\n",
        "\t\tmarks2sqmarks1[i] = (marks2sqmarks1[i] - meanmarks2sqmarks1) / (maxmarks2sqmarks1 - minmarks2sqmarks1)\n",
        "\n",
        "\tX_train = []\n",
        "\tX_test = []\n",
        "\tY_train = []\n",
        "\tY_test = []\n",
        "\n",
        "\tfor i in range(70):\n",
        "\t\tX_train.append([1, (marks1[i] - meanmarks1) / (maxmarks1 - minmarks1), (marks2[i] - meanmarks2) / (maxmarks2 - minmarks2), marks1sq[i], marks2sq[i], marks1marks2[i], marks1cu[i], marks2cu[i], marks1sqmarks2[i], marks2sqmarks1[i]])\n",
        "\t\tY_train.append(Y[i])\n",
        "\n",
        "\tfor i in range(71, 100):\n",
        "\t\tX_test.append([1, (marks1[i] - meanmarks1) / (maxmarks1 - minmarks1), (marks2[i] - meanmarks2) / (maxmarks2 - minmarks2), marks1sq[i], marks2sq[i], marks1marks2[i], marks1cu[i], marks2cu[i], marks1sqmarks2[i], marks2sqmarks1[i]])\n",
        "\t\tY_test.append(Y[i])\n",
        "\treturn X_train, X_test, Y_train, Y_test\n",
        "\n",
        "def sigmoid(z):\n",
        "\ttry:\n",
        "\t\tans = 1.0 / (1 + math.exp(-1 * z))\n",
        "\texcept OverflowError:\n",
        "\t\tans = 0\n",
        "\treturn ans\n",
        "\n",
        "# Function to calculate Slope to find coefficients\n",
        "def Slope(Coeff, X_train, Y_train, ind):\n",
        "\tdiff = 0\n",
        "\tfor i in range(len(X_train)):\n",
        "\t\titr = 0\n",
        "\t\tfor j in range(len(Coeff)):\n",
        "\t\t\titr = itr + Coeff[j] * X_train[i][j]\n",
        "\t\tdiff += (sigmoid(itr) - Y_train[i]) * X_train[i][ind]\n",
        "\treturn diff\n",
        "\n",
        "# Using batch gradient\n",
        "def batchgra(X_train, Y_train, alpha = 0.00001, epochs = 50000):\n",
        "\tLearningRateNoScaling = alpha\n",
        "\tCoeff = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "\tlis1 = []\n",
        "\tfor i in range(epochs):\n",
        "\t\tTempCoeff = Coeff.copy()\n",
        "\t\tfor j in range(len(Coeff)):\n",
        "\t\t\tTempCoeff[j] = TempCoeff[j] - ((LearningRateNoScaling / len(X_train)) * (Slope(Coeff, X_train, Y_train, j)))\n",
        "\t\tCoeff = TempCoeff.copy()\n",
        "\treturn Coeff\n",
        "\n",
        "# Finding Accuracy\n",
        "def printaccuracy(X_test, Y_test, Coeff):\n",
        "\tcount = 0\n",
        "\tfor i in range(len(X_test)):\n",
        "\t\tpredicted = 0\n",
        "\t\tfor j in range(len(Coeff)):\n",
        "\t\t  \tpredicted = predicted + Coeff[j] * X_test[i][j]\n",
        "\t\tpredicted = sigmoid(predicted)\n",
        "\t\tif predicted > 0.5:\n",
        "\t\t\tif Y_test[i] == 1:\n",
        "\t\t\t\tcount += 1\n",
        "\t\telse:\n",
        "\t\t\tif Y_test[i] == 0:\n",
        "\t\t\t\tcount += 1\n",
        "\tprint(\"Accuracy is : \" + str(count / len(Y_test) * 100))\n",
        "\n",
        "def SlopeStoch(Coeff, X_train, ActualVal, ind):\n",
        "\titr = 0\n",
        "\tfor j in range(len(Coeff)):\n",
        "\t\titr = itr + Coeff[j] * X_train[j]\n",
        "\treturn (sigmoid(itr) - ActualVal) * X_train[ind]\n",
        "\n",
        "def stochgra(X_train, Y_train, alpha = 0.00001, epochs = 50000):\n",
        "\tLearningRateNoScaling = alpha\n",
        "\tCoeff = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "\tfor iter in range(epochs):\n",
        "\t\tfor i in range(len(Y_train)):\n",
        "\t\t\tTempCoeff = Coeff.copy()\n",
        "\t\t\tfor j in range(len(Coeff)):\n",
        "\t\t\t\tTempCoeff[j] = TempCoeff[j] - (LearningRateNoScaling * (SlopeStoch(Coeff, X_train[i], Y_train[i], j)))\n",
        "\t\t\tCoeff = TempCoeff.copy()\n",
        "\treturn Coeff\n",
        "\n",
        "def minibtchgra(X_train, Y_train, alpha = 0.000000001, epochs = 30, batchsize = 20):\n",
        "\tLearningRateScaling = alpha\n",
        "\tCoeff = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "\tNoOfBatches = math.ceil(len(Y_train) / batchsize)\n",
        "\tequallyDiv = False\n",
        "\tif (len(Y_train) % batchsize == 0):\n",
        "\t\tequallyDiv = True;\n",
        "\n",
        "\tfor epoch in range(epochs):\n",
        "\t\tfor batch in range(NoOfBatches):\n",
        "\t\t\tSummation = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "\t\t\tfor j in range(len(Coeff)):\n",
        "\t\t\t\tfor i in range(batchsize):\n",
        "\t\t\t\t\tif (batch * batchsize + i == len(X_train)):\n",
        "\t\t\t\t\t\tbreak\n",
        "\t\t\t\t\tPredictedValue = 0.0\n",
        "\t\t\t\t\tfor wj in range(len(Coeff)):\n",
        "\t\t\t\t\t\tPredictedValue += Coeff[wj] * X_train[batch * batchsize + i][wj]\n",
        "\t\t\t\t\tPredictedValue = sigmoid(PredictedValue)\n",
        "\t\t\t\t\tPredictedValue -= Y_train[batch * batchsize + i]\n",
        "\t\t\t\t\tPredictedValue *= X_train[batch * batchsize + i][j]\n",
        "\n",
        "\t\t\t\t\tSummation[j] += PredictedValue;\n",
        "\n",
        "\t\t\tif (not equallyDiv and batch == NoOfBatches - 1):\n",
        "\t\t\t\tfor j in range(len(Summation)):\n",
        "\t\t\t\t\tCoeff[j] -= (Summation[j] / (len(Y_train) % batchsize)) * LearningRateScaling\n",
        "\t\t\telse:\n",
        "\t\t\t\tfor j in range(len(Summation)):\n",
        "\t\t\t\t\tCoeff[j] -= (Summation[j] / batchsize) * LearningRateScaling\n",
        "\treturn Coeff\n",
        "\n",
        "# Using batch gradient\n",
        "def batchgrareg(X_train, Y_train, alpha = 0.00001, epochs = 50000, lambdaparameter = -49):\n",
        "\tLearningRateNoScaling = alpha\n",
        "\n",
        "\tCoeff = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "\tlis1 = []\n",
        "\tfor i in range(epochs):\n",
        "\t\tTempCoeff = Coeff.copy()\n",
        "\t\tfor j in range(len(Coeff)):\n",
        "\t\t\tif j == 0:\n",
        "\t\t\t\tTempCoeff[j] = TempCoeff[j] - ((LearningRateNoScaling / len(X_train)) * (Slope(Coeff, X_train, Y_train, j)))\n",
        "\t\t\telse:\n",
        "\t\t\t\tTempCoeff[j] = (1 - alpha * lambdaparameter / len(X_train)) * TempCoeff[j] - ((LearningRateNoScaling / len(X_train)) * (Slope(Coeff, X_train, Y_train, j)))\n",
        "\t\tCoeff = TempCoeff.copy()\n",
        "\treturn Coeff\n",
        "\n",
        "def stochgrareg(X_train, Y_train, alpha = 0.00001, epochs = 50000, lambdaparameter = 1000):\n",
        "\tLearningRateNoScaling = alpha\n",
        "\tCoeff = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "\tfor iter in range(epochs):\n",
        "\t\tfor i in range(len(Y_train)):\n",
        "\t\t\tTempCoeff = Coeff.copy()\n",
        "\t\t\tfor j in range(len(Coeff)):\n",
        "\t\t\t\tif j == 0:\n",
        "\t\t\t\t\tTempCoeff[j] = TempCoeff[j] - (LearningRateNoScaling * (SlopeStoch(Coeff, X_train[i], Y_train[i], j)))\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tTempCoeff[j] = (1 - alpha * lambdaparameter) * TempCoeff[j] - (LearningRateNoScaling * (SlopeStoch(Coeff, X_train[i], Y_train[i], j)))\n",
        "\t\t\tCoeff = TempCoeff.copy()\n",
        "\treturn Coeff\n",
        "\n",
        "def minibtchgrareg(X_train, Y_train, alpha = 0.000000001, epochs = 30, batchsize = 20, LambdaParameter = 10):\n",
        "\tLearningRateScaling = alpha\n",
        "\tCoeff = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "\tNoOfBatches = math.ceil(len(Y_train) / batchsize)\n",
        "\tequallyDiv = False\n",
        "\tif (len(Y_train) % batchsize == 0):\n",
        "\t\tequallyDiv = True;\n",
        "\n",
        "\tfor epoch in range(epochs):\n",
        "\t\tfor batch in range(NoOfBatches):\n",
        "\t\t\tSummation = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "\t\t\tfor j in range(len(Coeff)):\n",
        "\t\t\t\tfor i in range(batchsize):\n",
        "\t\t\t\t\tif (batch * batchsize + i == len(X_train)):\n",
        "\t\t\t\t\t\tbreak\n",
        "\t\t\t\t\tPredictedValue = 0.0\n",
        "\t\t\t\t\tfor wj in range(len(Coeff)):\n",
        "\t\t\t\t\t\tPredictedValue += Coeff[wj] * X_train[batch * batchsize + i][wj]\n",
        "\t\t\t\t\tPredictedValue = sigmoid(PredictedValue)\n",
        "\t\t\t\t\tPredictedValue -= Y_train[batch * batchsize + i]\n",
        "\t\t\t\t\tPredictedValue *= X_train[batch * batchsize + i][j]\n",
        "\t\t\t\t\tSummation[j] += PredictedValue;\n",
        "\n",
        "\t\t\tif (not equallyDiv and batch == NoOfBatches - 1):\n",
        "\t\t\t\tfor j in range(len(Summation)):\n",
        "\t\t\t\t\tif j == 0:\n",
        "\t\t\t\t\t\tCoeff[j] = Coeff[j] - (Summation[j] / (len(Y_train) % batchsize)) * LearningRateScaling\n",
        "\t\t\t\t\telse:\n",
        "\t\t\t\t\t\tCoeff[j] = (1 - LearningRateScaling * LambdaParameter / (len(Y_test) % batchsize)) * Coeff[j] - (Summation[j] / (len(Y_train) % batchsize)) * LearningRateScaling\n",
        "\t\t\telse:\n",
        "\t\t\t\tfor j in range(len(Summation)):\n",
        "\t\t\t\t\tif j == 0:\n",
        "\t\t\t\t\t\tCoeff[j] = Coeff[j] - (Summation[j] / batchsize) * LearningRateScaling\n",
        "\t\t\t\t\telse:\n",
        "\t\t\t\t\t\tCoeff[j] = (1 - LearningRateScaling * LambdaParameter / batchsize) * Coeff[j] - (Summation[j] / batchsize) * LearningRateScaling\n",
        "\treturn Coeff\n",
        "\n",
        "# First doing batch gradient, stochaistic gradient and mini batch gradient without regularisation.\n",
        "X_train, X_test, Y_train, Y_test = getscaleddata()\n",
        "\n",
        "print(\"Doing batch gradient without regularisation\")\n",
        "coeff = batchgra(X_train, Y_train, 0.00001, 1000)\n",
        "print(coeff)\n",
        "printaccuracy(X_test, Y_test, coeff)\n",
        "\n",
        "print(\"Doing stochaistic gradient without regularisation\")\n",
        "coeff = stochgra(X_train, Y_train, 0.0001, 5000)\n",
        "print(coeff)\n",
        "printaccuracy(X_test, Y_test, coeff)\n",
        "\n",
        "print(\"Doing Mini batch gradient without regularisation\")\n",
        "coeff = minibtchgra(X_train, Y_train, 0.0001, 100, 32)\n",
        "print(coeff)\n",
        "printaccuracy(X_test, Y_test, coeff)\n",
        "\n",
        "# Now doing batch gradient, stochaistic gradient and mini batch gradient with regularisation.\n",
        "print(\"Doing batch gradient with regularisation\")\n",
        "coeff = batchgrareg(X_train, Y_train, 0.0001, 5000, 1000)\n",
        "print(coeff)\n",
        "printaccuracy(X_test, Y_test, coeff)\n",
        "\n",
        "print(\"Doing stochaistic gradient with regularisation\")\n",
        "coeff = stochgrareg(X_train, Y_train, 0.001, 500, 1000)\n",
        "print(coeff)\n",
        "printaccuracy(X_test, Y_test, coeff)\n",
        "\n",
        "print(\"Doing Mini batch gradient with regularisation\")\n",
        "coeff = minibtchgrareg(X_train, Y_train, 0.0001, 1000, 32, 1000)\n",
        "print(coeff)\n",
        "printaccuracy(X_test, Y_test, coeff)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDWrNkhPZSK1",
        "outputId": "23026250-52fa-4001-e006-f309d26fb3af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Doing batch gradient without regularisation\n",
            "[0.00014294109269907482, 0.0008466291272877591, 0.0006328899627740144, 0.0007928664267128853, 0.0006303380253779907, 0.0009439414551390919, 0.0007157627554145762, 0.0006008736216772345, 0.0008505735436931656, 0.0009386974687037283]\n",
            "Accuracy is : 86.20689655172413\n",
            "Doing stochaistic gradient without regularisation\n",
            "[0.4719748085008133, 1.266603181118865, 0.8734433801314477, 1.1108427309528415, 0.809090895300768, 1.404869142639804, 0.9277985971527912, 0.7183970059582026, 1.225177638068265, 1.3376390037180164]\n",
            "Accuracy is : 86.20689655172413\n",
            "Doing Mini batch gradient without regularisation\n",
            "[0.0003139158567546346, 0.0021139801895686894, 0.002286468359500325, 0.0020092454445606424, 0.002365904470532104, 0.0027402884892648734, 0.001829010812327897, 0.002346753584039545, 0.0023607218870855306, 0.002922762541773652]\n",
            "Accuracy is : 75.86206896551724\n",
            "Doing batch gradient with regularisation\n",
            "[0.006863560722146764, 0.005891396036649254, 0.004402230240029623, 0.005515685122947359, 0.004383279873095818, 0.006567957545079797, 0.004977713628862961, 0.004177306468807534, 0.005917273358103763, 0.006530089732814027]\n",
            "Accuracy is : 93.10344827586206\n",
            "Doing stochaistic gradient with regularisation\n",
            "[0.05682116114133792, 7.678638524627715e-06, -3.723372564837103e-05, -1.2273696835885114e-05, -5.5379895981841176e-05, -1.8190458673425044e-05, -2.94485642105931e-05, -6.831851949041436e-05, -2.278389703076626e-05, -4.376380862365612e-05]\n",
            "Accuracy is : 82.75862068965517\n",
            "Doing Mini batch gradient with regularisation\n",
            "[0.003057161092383144, 0.0021763017930656065, 0.002355427457767837, 0.002068798073992219, 0.0024374097198556367, 0.0028217932834047736, 0.0018834674412088585, 0.002417810801544484, 0.0024308480102274723, 0.003010013113226934]\n",
            "Accuracy is : 93.10344827586206\n"
          ]
        }
      ]
    }
  ]
}